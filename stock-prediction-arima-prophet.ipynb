{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9013514,"sourceType":"datasetVersion","datasetId":5430862}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/harasysodi/stock-prediction-arima-prophet?scriptVersionId=286238189\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Technical Description of the Machine Learning Project\n\nThis project aims to develop a robust machine learning model capable of forecasting the future share price of Hafez Tile. We will leverage historical stock data, harnessing the power of time-series analysis techniques to uncover patterns and trends that can inform our predictions. ","metadata":{"heading_collapsed":true}},{"cell_type":"markdown","source":"## Key Objectives:\n\nData Acquisition and Preparation: Collect and preprocess historical stock data for Hafez Tile, ensuring its quality and suitability for analysis.\nExploratory Data Analysis (EDA): Gain insights into the statistical properties of the data, identify potential outliers or anomalies, and visualize key trends and patterns.\nFeature Engineering : Create additional features, such as lagged variables or technical indicators, to potentially enhance model performance.\nModel Selection and Training: Train and evaluate a variety of time-series models, including both classical (ARIMA) and potentially deep learning approaches (LSTM).\nModel Evaluation and Selection: Compare the performance of different models using appropriate metrics (e.g., MAE, RMSE) and select the most accurate model for forecasting.\nVisualization and Interpretation: Present model predictions and relevant insights through clear and informative visualizations.\n\nThis project will focus exclusively on time-series data and quantitative modeling techniques. While fundamental analysis and other factors (such as news sentiment or macroeconomic trends) can play a significant role in stock price movements, they are beyond the scope of this initial investigation.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nimport statsmodels.api as sm\nfrom prophet import Prophet\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:00:30.18369Z","iopub.execute_input":"2024-07-24T00:00:30.184093Z","iopub.status.idle":"2024-07-24T00:00:35.550332Z","shell.execute_reply.started":"2024-07-24T00:00:30.184058Z","shell.execute_reply":"2024-07-24T00:00:35.548851Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"def load_all_data(data_directory=\"/kaggle/input/khafez-cl/KHAFEZ_CL.xlsx\"):\n    notebook_directory = os.getcwd()\n    data_directory = os.path.join(notebook_directory, data_directory)\n\n    dataframes = {}\n    loaded_dataframes_list = []\n\n    if data_directory.endswith('.xlsx'):\n        try:\n            df = pd.read_excel(data_directory)\n            key = os.path.splitext(os.path.basename(data_directory))[0]\n            dataframes[key] = df\n            loaded_dataframes_list.append(df)\n        except Exception as e:\n            print(f\"Error loading XLSX file {data_directory}: {e}\")\n    else:\n        print(f\"Unsupported file format: {data_directory}\")\n\n    print(\"Loaded DataFrames Summary:\")\n    for name, df in dataframes.items():\n        print(f\"- {name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n    return dataframes, loaded_dataframes_list\n\nall_data_dict, all_data_list = load_all_data()\n\n# Print the first few rows of the loaded dataframe\nif all_data_list:\n    print(\"\\nFirst few rows of the loaded dataframe:\")\n    print(all_data_list[0].head())\nelse:\n    print(\"No dataframes were loaded.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:00:35.552453Z","iopub.execute_input":"2024-07-24T00:00:35.553266Z","iopub.status.idle":"2024-07-24T00:00:36.345492Z","shell.execute_reply.started":"2024-07-24T00:00:35.553222Z","shell.execute_reply":"2024-07-24T00:00:36.344241Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"header_rows = {\n    'KHAFEZ_CL': 0,  # Header row for KHAFEZ_CL is row 0\n}\n\ndef clean_and_display_columns(dataframes, header_rows):\n    cleaned_dataframes = {}\n\n    for filename, df in dataframes.items():\n        # Display original column names\n        print(f\"\\nOriginal columns in '{filename}':\\n{df.columns.tolist()}\")\n\n        header_row = header_rows.get(filename, 0)  # Default to 0 if not specified\n\n        # Use the specified row as header\n        df.columns = df.iloc[header_row]\n        \n        # Remove rows before and including the header row\n        df = df.iloc[header_row + 1:].reset_index(drop=True)\n\n        # Rename columns\n        column_translations = {\n            '<TICKER>': 'Ticker',\n            '<DTYYYYMMDD>': 'Date',\n            '<FIRST>': 'First_Price',\n            '<HIGH>': 'High_Price',\n            '<LOW>': 'Low_Price',\n            '<CLOSE>': 'Close_Price',\n            '<VALUE>': 'Trade_Value',\n            '<VOL>': 'Volume',\n            '<OPENINT>': 'Open_Interest',\n            '<PER>': 'Price_Earnings_Ratio',\n            '<OPEN>': 'Open_Price',\n            '<LAST>': 'Last_Price'\n        }\n        df.rename(columns=column_translations, inplace=True)\n\n        cleaned_dataframes[filename] = df\n\n        # Display new column names\n        print(f\"\\nNew columns in '{filename}':\\n{df.columns.tolist()}\")\n\n    return cleaned_dataframes\n\ncleaned_dataframes = clean_and_display_columns(all_data_dict, header_rows)\n\n# Print the first few rows of the cleaned dataframe\nif cleaned_dataframes:\n    first_df_name = list(cleaned_dataframes.keys())[0]\n    print(f\"\\nFirst few rows of the cleaned dataframe '{first_df_name}':\")\n    print(cleaned_dataframes[first_df_name].head())\nelse:\n    print(\"No dataframes were cleaned.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:00:39.831125Z","iopub.execute_input":"2024-07-24T00:00:39.831881Z","iopub.status.idle":"2024-07-24T00:00:39.854599Z","shell.execute_reply.started":"2024-07-24T00:00:39.83184Z","shell.execute_reply":"2024-07-24T00:00:39.853006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cleaned_dataframes = clean_and_display_columns(all_data_dict, header_rows)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:00:47.865965Z","iopub.execute_input":"2024-07-24T00:00:47.867139Z","iopub.status.idle":"2024-07-24T00:00:47.87488Z","shell.execute_reply.started":"2024-07-24T00:00:47.867093Z","shell.execute_reply":"2024-07-24T00:00:47.873627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming we're working with the first (and only) dataframe in cleaned_dataframes\ndf = list(cleaned_dataframes.values())[0]\n\n# Convert Date column\ndf['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n\n# Sort by date\ndf = df.sort_values('Date').reset_index(drop=True)\n\n# Drop Price_Earnings_Ratio column if it exists\nif 'Price_Earnings_Ratio' in df.columns:\n    df = df.drop('Price_Earnings_Ratio', axis=1)\n\n# Identify trading days\ndf['Is_Trading_Day'] = ((df['First_Price'] != 0) | (df['High_Price'] != 0) | (df['Low_Price'] != 0) | (df['Volume'] != 0)).astype(int)\n\n# For non-trading days, fill price columns with the last available price\nprice_columns = ['First_Price', 'High_Price', 'Low_Price', 'Close_Price', 'Open_Price', 'Last_Price']\nfor col in price_columns:\n    df[col] = df[col].replace(0, np.nan)\ndf[price_columns] = df[price_columns].ffill()\n\n# Convert numeric columns\nnumeric_columns = ['First_Price', 'High_Price', 'Low_Price', 'Close_Price', 'Trade_Value', 'Volume', 'Open_Interest', 'Open_Price', 'Last_Price']\nfor col in numeric_columns:\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# Calculate returns and log returns\ndf['Returns'] = df['Close_Price'].pct_change()\ndf['Log_Returns'] = np.log(df['Close_Price'] / df['Close_Price'].shift(1))\n\n# Calculate volatility and moving averages\ndf['Volatility'] = df['Returns'].rolling(window=20).std()\ndf['MA_5'] = df['Close_Price'].rolling(window=5).mean()\ndf['MA_20'] = df['Close_Price'].rolling(window=20).mean()\n\n# Fill NaN values\ndf = df.fillna(method='ffill').fillna(method='bfill')\n\n# Print info about the preprocessed dataframe\nprint(df.info())\n\n# Display the first few rows to verify the changes\nprint(df.head())\n\n# Check for stationarity\nfrom statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(timeseries):\n    result = adfuller(timeseries, autolag='AIC')\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print(f'\\t{key}: {value}')\n\n# Check if we have enough data points\nif len(df) > 20:  # Assuming we need at least 20 data points for a meaningful test\n    test_stationarity(df['Close_Price'])\nelse:\n    print(\"Not enough data points for stationarity test\")\n\n# Analyze trading days (Saturday as first day of the week)\ndf['Day_of_Week'] = (df['Date'].dt.dayofweek + 1) % 7  # 0 = Saturday, 6 = Friday\ntrading_days_by_weekday = df[df['Is_Trading_Day'] == 1]['Day_of_Week'].value_counts().sort_index()\nprint(\"\\nTrading days by day of week:\")\nprint(trading_days_by_weekday)\n\n# Visualize trading days\nplt.figure(figsize=(10, 6))\ntrading_days_by_weekday.plot(kind='bar')\nplt.title('Number of Trading Days by Day of Week')\nplt.xlabel('Day of Week (0 = Saturday, 6 = Friday)')\nplt.ylabel('Number of Trading Days')\nplt.xticks(range(7), ['Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:02:22.450996Z","iopub.execute_input":"2024-07-24T00:02:22.451423Z","iopub.status.idle":"2024-07-24T00:02:23.074228Z","shell.execute_reply.started":"2024-07-24T00:02:22.451387Z","shell.execute_reply":"2024-07-24T00:02:23.072982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Stationarity Test:\n\nThe Augmented Dickey-Fuller test was performed on 'Close_Price'.\nThe p-value (0.0206) is less than 0.05, which suggests that we can reject the null hypothesis of non-stationarity.\nThis indicates that the 'Close_Price' series is likely stationary, which is good for many time series models.","metadata":{}},{"cell_type":"markdown","source":"# Visualisation","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15, 10))\n\n# Close Price\nplt.subplot(3, 1, 1)\nplt.plot(df['Date'], df['Close_Price'])\nplt.title('Close Price Over Time')\nplt.ylabel('Close Price')\n\n# Volume\nplt.subplot(3, 1, 2)\nplt.bar(df['Date'], df['Volume'], alpha=0.7)\nplt.title('Trading Volume Over Time')\nplt.ylabel('Volume')\n\n# Returns\nplt.subplot(3, 1, 3)\nplt.plot(df['Date'], df['Returns'])\nplt.title('Daily Returns Over Time')\nplt.ylabel('Returns')\n\nplt.tight_layout()\nplt.show()\n\n# Distribution of returns\nplt.figure(figsize=(10, 6))\nsns.histplot(df['Returns'], kde=True, bins=50)\nplt.title('Distribution of Daily Returns')\nplt.xlabel('Returns')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:07:24.973214Z","iopub.execute_input":"2024-07-24T00:07:24.973686Z","iopub.status.idle":"2024-07-24T00:07:32.337384Z","shell.execute_reply.started":"2024-07-24T00:07:24.973652Z","shell.execute_reply":"2024-07-24T00:07:32.336212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# EDA : Correlation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef calculate_correlation_stats(df, columns):\n    results = []\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            x = df[columns[i]]\n            y = df[columns[j]]\n            correlation, p_value = stats.pearsonr(x, y)\n            results.append({\n                'Variable 1': columns[i],\n                'Variable 2': columns[j],\n                'Correlation': correlation,\n                'P-value': p_value,\n                'Sample Size': len(x)\n            })\n    return pd.DataFrame(results)\n\n# Select numeric columns for correlation analysis\nnumeric_cols = ['Close_Price', 'Volume', 'Returns', 'Volatility', 'MA_5', 'MA_20']\n\n# Calculate correlation matrix\ncorrelation_matrix = df[numeric_cols].corr()\n\n# Calculate statistical measures\ncorrelation_stats = calculate_correlation_stats(df, numeric_cols)\n\n# Sort by absolute correlation value\ncorrelation_stats['Abs_Correlation'] = abs(correlation_stats['Correlation'])\ncorrelation_stats = correlation_stats.sort_values('Abs_Correlation', ascending=False)\n\n# Display correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n\n# Display correlation statistics\npd.set_option('display.float_format', '{:.4f}'.format)\nprint(\"\\nCorrelation Statistics:\")\nprint(correlation_stats[['Variable 1', 'Variable 2', 'Correlation', 'P-value', 'Sample Size']])\n\n# Identify significant correlations\nalpha = 0.05  # significance level\nsignificant_correlations = correlation_stats[\n    (correlation_stats['P-value'] < alpha) & \n    (abs(correlation_stats['Correlation']) > 0.5)\n]\n\nprint(\"\\nStatistically Significant and Strong Correlations:\")\nprint(significant_correlations[['Variable 1', 'Variable 2', 'Correlation', 'P-value']])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:09:47.178056Z","iopub.execute_input":"2024-07-24T00:09:47.178838Z","iopub.status.idle":"2024-07-24T00:09:47.659478Z","shell.execute_reply.started":"2024-07-24T00:09:47.1788Z","shell.execute_reply":"2024-07-24T00:09:47.658256Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Machine Learning : Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport pywt","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:15:25.488292Z","iopub.execute_input":"2024-07-24T00:15:25.489298Z","iopub.status.idle":"2024-07-24T00:15:25.494614Z","shell.execute_reply.started":"2024-07-24T00:15:25.48925Z","shell.execute_reply":"2024-07-24T00:15:25.493359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare the data for machine learning\ndf['Close_Price_shifted'] = df['Close_Price'].shift(-1)\ndf.dropna(inplace=True)\n\n# Features\nX = df[['Close_Price', 'Volume', 'Volatility', 'MA_5', 'MA_20']]\ny = df['Close_Price_shifted']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Train Random Forest model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)  # Fit on original data\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model (using original values)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nmape = mean_absolute_percentage_error(y_test, y_pred)\n\nprint('Mean Squared Error (MSE):', mse)\nprint('Root Mean Squared Error (RMSE):', rmse)\nprint('Mean Absolute Error (MAE):', mae)\nprint('Mean Absolute Percentage Error (MAPE):', mape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Machine Learning : ARIMA","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom pmdarima import auto_arima\n\n# Prepare the data\nfeatures = ['Volume', 'Returns', 'Volatility', 'MA_20', 'Is_Trading_Day']\nfor col in ['Volume', 'Returns']:\n    for lag in [1, 2, 3, 5]:\n        df[f'{col}_Lag_{lag}'] = df[col].shift(lag)\n        features.append(f'{col}_Lag_{lag}')\n\ndf = df.dropna()\n\n# Set the date as the index\ndf.set_index('Date', inplace=True)\n\n# Split the data\ntrain_size = int(len(df) * 0.8)\ntrain, test = df[:train_size], df[train_size:]\n\n# Scale the features\nscaler = StandardScaler()\ntrain_scaled = pd.DataFrame(scaler.fit_transform(train[features]), columns=features, index=train.index)\ntest_scaled = pd.DataFrame(scaler.transform(test[features]), columns=features, index=test.index)\n\n# Find the best SARIMA parameters\nauto_model = auto_arima(train['Close_Price'], exogenous=train_scaled, \n                        start_p=1, start_q=1, max_p=3, max_q=3, m=5,\n                        start_P=0, seasonal=True, d=1, D=1, trace=True,\n                        error_action='ignore', suppress_warnings=True, stepwise=True)\n\n# Fit the SARIMAX model\nmodel = SARIMAX(train['Close_Price'], exog=train_scaled, \n                order=auto_model.order, seasonal_order=auto_model.seasonal_order)\nresults = model.fit()\n\n# Make predictions\nforecast = results.get_forecast(steps=len(test), exog=test_scaled)\ny_pred = forecast.predicted_mean\n\n# Evaluate the model\nmse = mean_squared_error(test['Close_Price'], y_pred)\nmae = mean_absolute_error(test['Close_Price'], y_pred)\nr2 = r2_score(test['Close_Price'], y_pred)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"Mean Absolute Error:\", mae)\nprint(\"R-squared:\", r2)\n\n# Plot actual vs predicted values\nplt.figure(figsize=(12, 6))\nplt.plot(test.index, test['Close_Price'], label='Actual')\nplt.plot(test.index, y_pred, label='Predicted')\nplt.title('Actual vs Predicted Close Prices (SARIMAX)')\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.legend()\nplt.show()\n\n# Print model summary\nprint(results.summary())","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:22:08.060951Z","iopub.execute_input":"2024-07-24T00:22:08.061858Z","iopub.status.idle":"2024-07-24T00:25:22.075009Z","shell.execute_reply.started":"2024-07-24T00:22:08.061814Z","shell.execute_reply":"2024-07-24T00:25:22.073627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(timeseries):\n    result = adfuller(timeseries)\n    print('ADF Statistic:', result[0])\n    print('p-value:', result[1])\n    print('Critical Values:', result[4])\n\n# Check stationarity of differenced Close_Price\ndiff_close = df['Close_Price'].diff().dropna()\ntest_stationarity(diff_close)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:28:30.45454Z","iopub.execute_input":"2024-07-24T00:28:30.454977Z","iopub.status.idle":"2024-07-24T00:28:30.609287Z","shell.execute_reply.started":"2024-07-24T00:28:30.454944Z","shell.execute_reply":"2024-07-24T00:28:30.608083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ACF and PACF","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\nplot_acf(diff_close, ax=ax1, lags=40)\nplot_pacf(diff_close, ax=ax2, lags=40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:29:12.313007Z","iopub.execute_input":"2024-07-24T00:29:12.31392Z","iopub.status.idle":"2024-07-24T00:29:12.854548Z","shell.execute_reply.started":"2024-07-24T00:29:12.313862Z","shell.execute_reply":"2024-07-24T00:29:12.853359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Machine Learning : Prophet","metadata":{}},{"cell_type":"code","source":"pip install prophet","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:34:52.720806Z","iopub.execute_input":"2024-07-24T00:34:52.721273Z","iopub.status.idle":"2024-07-24T00:35:05.168465Z","shell.execute_reply.started":"2024-07-24T00:34:52.72124Z","shell.execute_reply":"2024-07-24T00:35:05.166819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from prophet import Prophet\nfrom prophet.plot import plot_plotly, add_changepoints_to_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\nimport pandas as pd\n\n# Prepare data for Prophet\nprophet_data = df.reset_index()[['Date', 'Close_Price']].rename(columns={'Date': 'ds', 'Close_Price': 'y'})\n\n# Split data\ntrain_size = int(len(prophet_data) * 0.8)\ntrain_prophet = prophet_data[:train_size]\ntest_prophet = prophet_data[train_size:]\n\n# Fit Prophet model\nmodel = Prophet(changepoint_prior_scale=0.05, yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\nmodel.fit(train_prophet)\n\n# Make predictions\nfuture = model.make_future_dataframe(periods=len(test_prophet))\nforecast = model.predict(future)\n\n# Evaluate the model\ny_pred = forecast['yhat'][-len(test_prophet):]\nmse = mean_squared_error(test_prophet['y'], y_pred)\nmae = mean_absolute_error(test_prophet['y'], y_pred)\nr2 = r2_score(test_prophet['y'], y_pred)\n\nprint(\"Prophet Model Results:\")\nprint(\"Mean Squared Error:\", mse)\nprint(\"Mean Absolute Error:\", mae)\nprint(\"R-squared:\", r2)\n\n# Plot results with uncertainty intervals\nfig = model.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), model, forecast)\nplt.title('Prophet Forecast with Changepoints')\nplt.show()\n\n# Plot components\nfig = model.plot_components(forecast)\nplt.show()\n\n# Plot actual vs predicted\nplt.figure(figsize=(12, 6))\nplt.plot(test_prophet['ds'], test_prophet['y'], label='Actual')\nplt.plot(test_prophet['ds'], y_pred, label='Predicted')\nplt.fill_between(test_prophet['ds'], \n                 forecast['yhat_lower'][-len(test_prophet):],\n                 forecast['yhat_upper'][-len(test_prophet):],\n                 alpha=0.3)\nplt.title('Actual vs Predicted Close Prices (Prophet)')\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.legend()\nplt.show()\n\n# Print the most important changepoints\nchangepoints = model.changepoints\nchangepoint_importance = np.abs(model.params['delta'])\n\nprint(\"Shape of changepoint_importance:\", changepoint_importance.shape)\nprint(\"Type of changepoint_importance:\", type(changepoint_importance))\nprint(\"First few values of changepoint_importance:\", changepoint_importance[:5])\n\n# Ensure changepoint_importance is 1-dimensional\nif changepoint_importance.ndim > 1:\n    changepoint_importance = changepoint_importance.flatten()\n\nprint(\"Shape after flattening:\", changepoint_importance.shape)\n\ntop_changepoints = sorted(zip(changepoints, changepoint_importance), key=lambda x: x[1], reverse=True)[:10]\nprint(\"\\nTop 10 changepoints:\")\nfor date, importance in top_changepoints:\n    print(f\"Date: {date}, Importance: {importance}\")\n\n# Plot changepoint importance\nplt.figure(figsize=(12, 6))\nplt.bar(range(len(changepoint_importance)), changepoint_importance)\nplt.title('Changepoint Importance')\nplt.xlabel('Changepoint Index')\nplt.ylabel('Importance')\nplt.show()\n\n# Analyze residuals\nresiduals = test_prophet['y'].values - y_pred\nplt.figure(figsize=(12, 6))\nplt.plot(test_prophet['ds'], residuals)\nplt.title('Residuals Over Time')\nplt.xlabel('Date')\nplt.ylabel('Residual')\nplt.show()\n\n# Plot residual distribution\nplt.figure(figsize=(12, 6))\nsns.histplot(residuals, kde=True)\nplt.title('Distribution of Residuals')\nplt.xlabel('Residual')\nplt.show()\n\n# Autocorrelation of residuals\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(12, 6))\nplot_acf(residuals, lags=40)\nplt.title('Autocorrelation of Residuals')\nplt.show()\n\n# Cross-correlation between actual and predicted\nfrom scipy import signal\n\ncorrelation = signal.correlate(test_prophet['y'], y_pred, mode='full')\nlags = signal.correlation_lags(len(test_prophet['y']), len(y_pred), mode='full')\nplt.figure(figsize=(12, 6))\nplt.plot(lags, correlation)\nplt.title('Cross-correlation between Actual and Predicted')\nplt.xlabel('Lag')\nplt.ylabel('Correlation')\nplt.show()\n\n# Analyze prediction intervals\ncoverage = np.mean((forecast['yhat_lower'][-len(test_prophet):] <= test_prophet['y']) & \n                   (test_prophet['y'] <= forecast['yhat_upper'][-len(test_prophet):]))\nprint(f\"\\nPrediction Interval Coverage: {coverage:.2%}\")\n\n# Compute rolling forecast\nwindow = 30  # 30-day rolling window\nrolling_mse = []\nfor i in range(len(test_prophet) - window):\n    y_true = test_prophet['y'][i:i+window]\n    y_pred = forecast['yhat'][-(len(test_prophet)-i):][:window]\n    rolling_mse.append(mean_squared_error(y_true, y_pred))\n\nplt.figure(figsize=(12, 6))\nplt.plot(test_prophet['ds'][window:], rolling_mse)\nplt.title(f'{window}-day Rolling MSE')\nplt.xlabel('Date')\nplt.ylabel('MSE')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T00:48:43.244183Z","iopub.execute_input":"2024-07-24T00:48:43.244686Z","iopub.status.idle":"2024-07-24T00:48:49.255157Z","shell.execute_reply.started":"2024-07-24T00:48:43.244653Z","shell.execute_reply":"2024-07-24T00:48:49.253979Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}